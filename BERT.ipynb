{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22aaa206-ee0c-4e76-8952-39c36b72e150",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gumpun\\AppData\\Local\\Temp\\ipykernel_22756\\196635466.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.optim import AdamW\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a179119-4aab-4e82-9b9a-9bd87463eb79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def file2list(fname):\n",
    "    with open(fname, encoding='utf-8') as fp:  # Specify the encoding\n",
    "        lines = fp.readlines()\n",
    "        return [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6c97e4-88ef-4dbf-9165-236400dd4fe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_text = (\"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/DataReviewProductThai/Positive.csv\")\n",
    "neg_text = (\"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/DataReviewProductThai/Negative.csv\")\n",
    "neu_text = (\"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/DataReviewProductThai/Neutrally.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e970c4b-3027-4b89-8174-7dbb9c85221b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Message</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>*******765</td>\n",
       "      <td>เสื้อตัวเล็กและผ้าบางมากๆๆๆๆ</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>0***8</td>\n",
       "      <td>บางไปนิดค่ะ</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>ภากมล แ.</td>\n",
       "      <td>สั่งสีขาวจีได้สีส้มแป๊ด ปวดหัว ร้านค้าตอบกลับด...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>จันทนา ใ.</td>\n",
       "      <td>สินค้าเล็กมากๆๆๆๆๆๆๆใส่ไม่ได้</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>*******644</td>\n",
       "      <td>เนื้อผ้าไม่โอเคไม่ผ่าน โหลมาก</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0***5</td>\n",
       "      <td>ส่งสินค้าเร๋วมากๆๆๆ รองเท้านุ่ม ใส่ดีสมราคาค่ะ</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Thanakorn N.</td>\n",
       "      <td>เนื้องานด้านนอกดูโอเค แต่ข้างในไม่เนียน กระเป๋...</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>ร***.</td>\n",
       "      <td>กระทะใบใหญ่ดี มีรอยบุบนิดหน่อย รอยเหมือนๆรูปใน...</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>สุชาติ จ.</td>\n",
       "      <td>การเคลื่อนไหวที่เงียบสงบและราบรื่น ดีไซน์ที่ดู...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>WORAPON S.</td>\n",
       "      <td>คุณภาพสมราคา ไม่ได้ดีมาก</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              User                                            Message  \\\n",
       "289     *******765                       เสื้อตัวเล็กและผ้าบางมากๆๆๆๆ   \n",
       "819          0***8                                        บางไปนิดค่ะ   \n",
       "758       ภากมล แ.  สั่งสีขาวจีได้สีส้มแป๊ด ปวดหัว ร้านค้าตอบกลับด...   \n",
       "696      จันทนา ใ.                      สินค้าเล็กมากๆๆๆๆๆๆๆใส่ไม่ได้   \n",
       "986     *******644                      เนื้อผ้าไม่โอเคไม่ผ่าน โหลมาก   \n",
       "...            ...                                                ...   \n",
       "24           0***5     ส่งสินค้าเร๋วมากๆๆๆ รองเท้านุ่ม ใส่ดีสมราคาค่ะ   \n",
       "554   Thanakorn N.  เนื้องานด้านนอกดูโอเค แต่ข้างในไม่เนียน กระเป๋...   \n",
       "528          ร***.  กระทะใบใหญ่ดี มีรอยบุบนิดหน่อย รอยเหมือนๆรูปใน...   \n",
       "32       สุชาติ จ.  การเคลื่อนไหวที่เงียบสงบและราบรื่น ดีไซน์ที่ดู...   \n",
       "1177    WORAPON S.                           คุณภาพสมราคา ไม่ได้ดีมาก   \n",
       "\n",
       "      Sentiment  \n",
       "289   Neutrally  \n",
       "819   Neutrally  \n",
       "758    Negative  \n",
       "696    Negative  \n",
       "986    Negative  \n",
       "...         ...  \n",
       "24     Positive  \n",
       "554   Neutrally  \n",
       "528   Neutrally  \n",
       "32     Positive  \n",
       "1177  Neutrally  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = pd.read_csv(pos_text, delimiter=',', encoding='utf-8').dropna()\n",
    "neg_df = pd.read_csv(neg_text, delimiter=',', encoding='utf-8').dropna()\n",
    "neu_df = pd.read_csv(neu_text, delimiter=',', encoding='utf-8').dropna()\n",
    "\n",
    "data = pd.concat([pos_df, neg_df, neu_df])\n",
    "data.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e54c327-76f2-4ff2-9488-9bc1bec2344a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive     1499\n",
       "Negative     1496\n",
       "Neutrally    1496\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts = data['Sentiment'].value_counts()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f34c495-76c8-4288-ac7a-7072c7e53dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pythainlp in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from pythainlp) (2.31.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from pythainlp) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages (from requests>=2.22.0->pythainlp) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gumpun\\anaconda3\\envs\\py310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8ee685-bf13-4535-b5c8-6cfb306931d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5405d397-10f1-4339-8e3c-13093ba995e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Message</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>น***.</td>\n",
       "      <td>ได้ ของ ไม่ ครบ   สั่ง   ได้ แค่   ตัว   ไม่ ไ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>Natnaree2544</td>\n",
       "      <td>สั่ง ไป   ตัว แต่ ได้รับ มา แค่   ตัว และ ไม่ ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>สมศักดิ์</td>\n",
       "      <td>ของดี สั่ง รอบ แล้ว</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>ร***.</td>\n",
       "      <td>ใส่ ได้ พอดี คะ เนื้อผา ดีมาก คะ</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0***9</td>\n",
       "      <td>ไม่ ใส่ เลย คะ   สินค้า เหมาะส กับ ราค</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>มุทิตา ห.</td>\n",
       "      <td>สินค้า ไม่ ตรง ปก ผ้า บาง มาก เย็บ โหล ไม่ ดี ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>Pusanaporm</td>\n",
       "      <td>พอใช้ได   ตาม ราค ค่ะ</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Tnat T.</td>\n",
       "      <td>สี ไม่ เหมือนกับ รอย ก่อน   แถม บาง กว่า</td>\n",
       "      <td>Neutrally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>Piyathida</td>\n",
       "      <td>จัดส่ง เร็ว พอสมควร   ผ้า เบา สบาย ใน การเคลื่...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>ภคินี โ.</td>\n",
       "      <td>ผ้า บาง มาก</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              User                                            Message  \\\n",
       "1122         น***.  ได้ ของ ไม่ ครบ   สั่ง   ได้ แค่   ตัว   ไม่ ไ...   \n",
       "880   Natnaree2544  สั่ง ไป   ตัว แต่ ได้รับ มา แค่   ตัว และ ไม่ ...   \n",
       "883       สมศักดิ์                                ของดี สั่ง รอบ แล้ว   \n",
       "806          ร***.                   ใส่ ได้ พอดี คะ เนื้อผา ดีมาก คะ   \n",
       "314          0***9             ไม่ ใส่ เลย คะ   สินค้า เหมาะส กับ ราค   \n",
       "1313     มุทิตา ห.  สินค้า ไม่ ตรง ปก ผ้า บาง มาก เย็บ โหล ไม่ ดี ...   \n",
       "1331    Pusanaporm                              พอใช้ได   ตาม ราค ค่ะ   \n",
       "517        Tnat T.           สี ไม่ เหมือนกับ รอย ก่อน   แถม บาง กว่า   \n",
       "839      Piyathida  จัดส่ง เร็ว พอสมควร   ผ้า เบา สบาย ใน การเคลื่...   \n",
       "237       ภคินี โ.                                        ผ้า บาง มาก   \n",
       "\n",
       "      Sentiment  \n",
       "1122   Negative  \n",
       "880    Negative  \n",
       "883    Positive  \n",
       "806   Neutrally  \n",
       "314    Negative  \n",
       "1313   Negative  \n",
       "1331  Neutrally  \n",
       "517   Neutrally  \n",
       "839    Positive  \n",
       "237    Negative  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# สร้างฟังก์ชันสำหรับการลบอักขระที่ซ้ำกันในแต่ละคำเท่านั้น\n",
    "def remove_duplicate_chars(text):\n",
    "    if isinstance(text, str):  # ตรวจสอบว่าข้อความไม่ใช่ NaN\n",
    "        unique_words = []\n",
    "        words = word_tokenize(text)\n",
    "        for word in words:\n",
    "            unique_word = ''\n",
    "            for char in word:\n",
    "                if char not in unique_word:\n",
    "                    unique_word += char\n",
    "            unique_words.append(unique_word)\n",
    "        return ' '.join(unique_words)\n",
    "    else:\n",
    "        return text  # ส่งค่า NaN กลับหากเป็น NaN\n",
    "\n",
    "# ใช้ฟังก์ชัน `remove_duplicate_chars` เพื่อ Tokenize และลบคำที่ซ้ำ\n",
    "data['Message'] = data['Message'].apply(remove_duplicate_chars)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6953b013-4323-40dc-9005-c2a1cbf27f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [2 2 2 ... 1 1 1]\n",
      "Mapping of encoded labels to original labels:\n",
      "0: Negative\n",
      "1: Neutrally\n",
      "2: Positive\n"
     ]
    }
   ],
   "source": [
    "Message = data['Message'].values\n",
    "Sentiment = data['Sentiment'].values\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(Sentiment)\n",
    "print(\"Encoded labels:\", encoded_labels)\n",
    "print(\"Mapping of encoded labels to original labels:\")\n",
    "\n",
    "for label, original_label in enumerate(encoder.classes_):\n",
    "    print(f\"{label}: {original_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff91332-103d-4ec0-b6cc-3ef648c181de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# แบ่งข้อมูลเป็นชุดฝึกและชุดทดสอบ\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(Message, encoded_labels, stratify=encoded_labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1123c15a-6d1d-49a0-a59c-891736ceb4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db490af6-a8bb-497a-b5c9-618494d84da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device).long()  # Convert to Long\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f0f40c5-6fd6-4092-934c-489b65e0d478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ฟังก์ชันนี้ใช้สำหรับประเมินผลลัพธ์ของโมเดลที่ถูกฝึกสอน\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f50117e-93e9-4e8a-ba0f-129938ddfbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " bert_model_name = 'bert-base-uncased'\n",
    " num_classes = 3\n",
    " max_length = 128\n",
    " batch_size = 16\n",
    " num_epochs = 10\n",
    " learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2964d90-ffc5-4b1e-960c-631570f47915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#คลาสที่ใช้สำหรับสร้าง Dataset สำหรับงานการจำแนกประเภทข้อความ\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e08ebc27-99ba-4dbc-b183-5602a8804a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#โค้ดที่ให้มากำลังดึง tokenizer และสร้าง Dataset และ DataLoader สำหรับการฝึกและการทดสอบโมเดล\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextClassificationDataset(train_sentences, train_labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TextClassificationDataset(test_sentences, test_labels, tokenizer, max_length)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9e100a3-9c0e-4d3b-91b4-578398636756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09326db2-e9c2-4b08-841e-c3d902b7e787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BERTClassifier(bert_model_name, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29b1da49-9848-4118-9da7-0b2e90ffbca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#การกำหนด optimizer และ scheduler ในการฝึกโมเดล BERT\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a8247b0-a782-490e-ae65-2c8396a9ac14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Validation Accuracy: 0.4756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.42      0.43       150\n",
      "           1       0.49      0.26      0.34       150\n",
      "           2       0.50      0.75      0.60       150\n",
      "\n",
      "    accuracy                           0.48       450\n",
      "   macro avg       0.47      0.48      0.45       450\n",
      "weighted avg       0.47      0.48      0.45       450\n",
      "\n",
      "Epoch 2/10\n",
      "Validation Accuracy: 0.5378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.64      0.55       150\n",
      "           1       0.50      0.35      0.41       150\n",
      "           2       0.64      0.62      0.63       150\n",
      "\n",
      "    accuracy                           0.54       450\n",
      "   macro avg       0.54      0.54      0.53       450\n",
      "weighted avg       0.54      0.54      0.53       450\n",
      "\n",
      "Epoch 3/10\n",
      "Validation Accuracy: 0.5422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.58      0.54       150\n",
      "           1       0.51      0.40      0.45       150\n",
      "           2       0.62      0.65      0.63       150\n",
      "\n",
      "    accuracy                           0.54       450\n",
      "   macro avg       0.54      0.54      0.54       450\n",
      "weighted avg       0.54      0.54      0.54       450\n",
      "\n",
      "Epoch 4/10\n",
      "Validation Accuracy: 0.5511\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.73      0.58       150\n",
      "           1       0.56      0.38      0.45       150\n",
      "           2       0.68      0.54      0.60       150\n",
      "\n",
      "    accuracy                           0.55       450\n",
      "   macro avg       0.57      0.55      0.55       450\n",
      "weighted avg       0.57      0.55      0.55       450\n",
      "\n",
      "Epoch 5/10\n",
      "Validation Accuracy: 0.5711\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.65      0.58       150\n",
      "           1       0.60      0.39      0.47       150\n",
      "           2       0.62      0.67      0.64       150\n",
      "\n",
      "    accuracy                           0.57       450\n",
      "   macro avg       0.58      0.57      0.56       450\n",
      "weighted avg       0.58      0.57      0.56       450\n",
      "\n",
      "Epoch 6/10\n",
      "Validation Accuracy: 0.6000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.52      0.54       150\n",
      "           1       0.59      0.53      0.55       150\n",
      "           2       0.65      0.75      0.70       150\n",
      "\n",
      "    accuracy                           0.60       450\n",
      "   macro avg       0.60      0.60      0.60       450\n",
      "weighted avg       0.60      0.60      0.60       450\n",
      "\n",
      "Epoch 7/10\n",
      "Validation Accuracy: 0.5822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.55      0.55       150\n",
      "           1       0.52      0.57      0.54       150\n",
      "           2       0.71      0.63      0.67       150\n",
      "\n",
      "    accuracy                           0.58       450\n",
      "   macro avg       0.59      0.58      0.58       450\n",
      "weighted avg       0.59      0.58      0.58       450\n",
      "\n",
      "Epoch 8/10\n",
      "Validation Accuracy: 0.5889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.58      0.56       150\n",
      "           1       0.54      0.55      0.54       150\n",
      "           2       0.69      0.64      0.66       150\n",
      "\n",
      "    accuracy                           0.59       450\n",
      "   macro avg       0.59      0.59      0.59       450\n",
      "weighted avg       0.59      0.59      0.59       450\n",
      "\n",
      "Epoch 9/10\n",
      "Validation Accuracy: 0.5844\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.53      0.54       150\n",
      "           1       0.54      0.53      0.54       150\n",
      "           2       0.66      0.69      0.67       150\n",
      "\n",
      "    accuracy                           0.58       450\n",
      "   macro avg       0.58      0.58      0.58       450\n",
      "weighted avg       0.58      0.58      0.58       450\n",
      "\n",
      "Epoch 10/10\n",
      "Validation Accuracy: 0.5956\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.53      0.55       150\n",
      "           1       0.55      0.55      0.55       150\n",
      "           2       0.68      0.70      0.69       150\n",
      "\n",
      "    accuracy                           0.60       450\n",
      "   macro avg       0.59      0.60      0.59       450\n",
      "weighted avg       0.59      0.60      0.59       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train(model, train_dataloader, optimizer, scheduler, device)\n",
    "        accuracy, report = evaluate(model, val_dataloader, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e1867cb-0b94-48b6-9252-0308908a2323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"C:/Users/Gumpun/Sentiment-Analysis-System-for-Consumer-Products/bert_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "073c416f-c681-4575-b672-c1a6ab876849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "    sentiment_mapping = {0: \"Negative\", 1: \"Neutrally\", 2: \"Positive\"}\n",
    "    return sentiment_mapping[preds.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f95cf16-a4fb-4cb3-aba5-b57cab1957e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: Neutrally\n"
     ]
    }
   ],
   "source": [
    "test_text = remove_duplicate_chars(\"เบื่อสินค้านี้มาก\")\n",
    "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(f\"Predicted sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d149ac8-1039-4bb0-8fed-217f74de94a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['เบื่อ', 'สินค้า', 'นี้', 'มาก']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = word_tokenize(remove_duplicate_chars(test_text), keep_whitespace=False)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2b18b75-767e-4915-a515-38bc0356b50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เบื่อ\n",
      "Predicted sentiment: Neutrally\n",
      "----------------------\n",
      "สินค้า\n",
      "Predicted sentiment: Neutrally\n",
      "----------------------\n",
      "นี้\n",
      "Predicted sentiment: Neutrally\n",
      "----------------------\n",
      "มาก\n",
      "Predicted sentiment: Negative\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized_text:\n",
    "    print(i)\n",
    "    test_text = remove_duplicate_chars(i)\n",
    "    sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "    print(f\"Predicted sentiment: {sentiment}\")\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e96c7-f0fc-4590-8590-e1fddc082549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
